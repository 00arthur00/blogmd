title: thanos设计
tags:
  - thanos
  - 监控
categories: []
date: 2019-03-26 13:48:00
---
[原文](https://github.com/improbable-eng/thanos/blob/master/docs/design.md)

# 设计

Thanos是一组组件，可以组成具有长期存储功能的高可用性Prometheus设置。 其主要目标是简化操作，保留Prometheus的可靠性。

Prometheus指标数据模型和2.0存储格式（[规范][tsdb-format]，[幻灯片][tsdb-talk]）是系统中所有组件的基础层。


## 架构

Thanos是具有鲜明(distinct)和解耦(decoupled)目的而描述组件的群集系统。 群集组件可以分类如下：

* 指标来源 Metric sources
* 存储 Stores
* 查询 Queriers

### 指标来源

数据源是生成或收集指标数据的组件的非常通用的定义。数据源将群集中的数据通告给潜在客户。可以通过众所周知的gRPC服务检索指标数据。

Thanos提供了两个充当数据源的组件：Prometheus边车和规则节点。

边车基于Prometheus的[HTTP and remote-read APIs][prom-http-api]实现gRPC服务。规则节点直接基于Prometheus存储引擎实现的。

#### 指标数据备份

数据源持久保存数据通过Prometheus 2.0存储引擎实现。存储引擎定期生成固定时间范围内的不可变数据块。块是一个包含少量较大文件的目录，其中包含检索数据所需的所有样本数据和必需索引：
```
01BX6V6TY06G5MFQ0GPH7EMXRH
├── chunks
│   ├── 000001
│   ├── 000002
│   └── 000003
├── index
└── meta.json
```


块顶级目录是ULID（如UUID，但按字典顺序排序并编码创建时间）。

* 块文件(chunk files)每个容纳几百MB的块。相同序列的块按顺序对齐。返回的系列按其指标名称对齐。这使下面变得更加相关。
* 索引(index)文件包含通过标签及其块的位置查找特定系列所需的所有信息。
* `meta.json`包含有关统计数据，时间范围和压缩级别等块的元信息。


这些块文件可以备份到对象存储，然后由另一个组件查询（参见下文）。

所有数据都是在Prometheus服务器/存储引擎创建时上传的。`meta.json`文件可以由`thanos`部分扩展，可以添加Thanos特定的元数据。目前，它包括块的生产者已分配的“外部标签”。这稍后有助于过滤用于查询的块而无需访问其数据文件。

meta.json在侧车上传时间内更新。


```
┌────────────┬─────────┐         ┌────────────┬─────────┐     ┌─────────┐
│ Prometheus │ Sidecar │   ...   │ Prometheus │ Sidecar │     │   Rule  │
└────────────┴────┬────┘         └────────────┴────┬────┘     └┬────────┘
                  │                                │           │
                Blocks                           Blocks      Blocks
                  │                                │           │
                  v                                v           v
              ┌──────────────────────────────────────────────────┐
              │                   Object Storage                 │
              └──────────────────────────────────────────────────┘
```

### 存储(stores)

存储节点充当在对象存储桶中的数据的数据块网关。它实现与数据源相同的gRPC API，以提供对存储桶中的所有指标数据的访问。

它不断同步存储桶中存在的块，并将对指标数据的请求转换为对象存储请求。它实现了各种策略以最小化对对象存储的请求的数量，例如通过其元数据（例如，时间范围和标签）过滤相关块并缓存频繁的索引查找。

Prometheus 2.0存储布局经过优化，可实现最小的读取放大。例如，相同时间序列的样本数据在块文件中顺序对齐。类似地，同一指标名称的序列也按顺序对齐。

存储节点知道文件的布局，并将数据请求转换为最小量的对象存储请求的计划。每个请求可以同时获取多达数十万个块。这对于满足对对象存储的有限量请求的大查询至关重要。

目前只缓存索引数据。块数据可以缓存，但大小要大几个数量级。在当前状态下，从对象存储中获取块数据已经只占端到端延迟的一小部分。因此，目前没有动力通过添加块缓存来增加存储节点资源需求/限制其可伸缩性。

### 存储和数据源 - 一切都是一样的

由于存储节点和数据源公开相同的gRPC Store API，因此客户端可以在很大程度上将它们视为等效的，并且不必关心它们正在查询的特定组件。

Store API的每个实现者都会公布有关其提供的数据的元信息。 这允许客户端最小化它们必须扇出的节点集，以满足特定的数据查询。

从本质上讲，Store API允许通过一组标签匹配器（从PromQL中获知）和时间范围来查找数据。 它返回在块数据中找到的压缩样本块。 它纯粹是一种数据检索API，并且不提供复杂的查询执行。

```
┌──────────────────────┐  ┌────────────┬─────────┐   ┌────────────┐
│ Google Cloud Storage │  │ Prometheus │ Sidecar │   │    Rule    │
└─────────────────┬────┘  └────────────┴────┬────┘   └─┬──────────┘
                  │                         │          │
         Block File Ranges                  │          │
                  │                     Store API      │
                  v                         │          │
                ┌──────────────┐            │          │
                │     Store    │            │      Store API
                └────────┬─────┘            │          │
                         │                  │          │
                     Store API              │          │
                         │                  │          │
                         v                  v          v
                       ┌──────────────────────────────────┐
                       │              Client              │
                       └──────────────────────────────────┘

```


### 查询器层(querier layer)

查询器是无状态和水平可伸缩的实例，它们在集群中公开的Store API之上实现PromQL。查询者参与群集以便能够弹性地发现所有数据源和存储节点。规则节点反过来可以发现查询节点以评估记录和警报规则。

基于存储节点和源节点的元数据，它们尝试最小化请求扇出以获取特定查询的数据。

```
┌──────────────────┐  ┌────────────┬─────────┐   ┌────────────┐
│    Store Node    │  │ Prometheus │ Sidecar │   │    Rule    │
└─────────────┬────┘  └────────────┴────┬────┘   └─┬──────────┘
              │                         │          │
              │                         │          │
              │                         │          │
              v                         v          v
        ┌─────────────────────────────────────────────────────┐
        │                      Query layer                    │
        └─────────────────────────────────────────────────────┘
                ^                  ^                  ^
                │                  │                  │
       ┌────────┴────────┐  ┌──────┴─────┐       ┌────┴───┐
       │ Alert Component │  │ Dashboards │  ...  │ Web UI │
       └─────────────────┘  └────────────┘       └────────┘

```

### 压缩器(compactor)

压缩器是一个单独的进程，不参与Thanos集群。 相反，它仅指向对象存储桶并且不断地将多个较小的块合并为较大的块。 这显着减少了存储区中的总存储大小，存储节点上的负载以及从存储区获取查询数据所需的请求数量。

将来，压缩器可以执行其他批处理，例如降采样和应用保留策略。

## 扩展

Thanos组件都没有提供任何分片方法。唯一可显式扩展的组件是查询节点，它们是无状态的并且可以任意扩展。通过依赖外部对象存储系统来确保存储容量的扩展。

存储(store)，规则(rule)和压缩器(compactor)节点都可在单个实例或HA对中显式扩展。与Prometheus类似，功能分片可以应用于罕见的情况，但这种情况并不适用。

例如，规则集可以跨多个HA对规则节点划分。无论是否为每个区域/日期中心分配专用存储桶，存储节点都可能受功能分片的影响。

总的来说，一流(first-class)的水平分片是可能的，但暂时不会考虑，因为没有证据表明它在实际设置中是必需的。


## 成本

Thanos增加现有Prometheus设置的唯一额外成本实际上是存储和查询来自对象存储和运行存储节点的数据的代价。

查询器，压缩器和规则节点需要的计算资源与通过不直接在Prometheus服务器上执行相同工作而节省的计算资源相同。

在传统的Prometheus设置中本地访问的数据必须通过Thanos网络传输。 我们通常期望这种数据混乱通常发生在未计量的网络中，因此不会导致任何额外的成本。

每GB的典型对象存储价格约为0.02美元。存储节点的检索数量（通常价格为每10,0000字节 0.004美元）很大程度上取决于各个查询模式。将总存储成本增加20％以考虑检索和运行存储节点似乎是一个保守的估计。

假设我们要存储100TB的度量数据。总数据大小约为1.07字节/样本，这相当于：
* 在平均100万个活跃时间序列中存储48.88年的数据，默认的15秒刮擦间隔。
* 在平均100万个活跃时间序列中存储3.25年的数据，并且具有1秒的刮擦间隔。

<details>
<summary>计算</summary>
<br>
用1.07字节/样品存储100TB。
<br>
100(TB)/1.07(字节/样本)=1.027580961×10¹⁴ 样本.
<br>
我们假设平均1mln时间序列，因此单个序列的102758096.1个"可用"样本以适应整体100TB。
<br>
15s刮擦间隔（4个样品/分钟）：
<br>
102758096.1(样本)/4(样本/分钟)=25689524.025分钟=~48.88年
<br>
1s刮擦间隔（60个样品/分钟）：
<br>
102758096.1(样本)/60(样本/分钟)=1712634.935分钟=~3.25年
</details>


在基线Prometheus设置之上，此数量的度量数据的成本将花费大约2400美元/月。

作为回报，能够将Prometheus实例的保留时间从数周缩短至数小时，可为本地SSD或网络块存储（通常为0.17美元/ GB）节省成本，并减少内存消耗。

此计算尚未考虑较低优先级数据和降采样的较短保留范围。

[tsdb-format]: https://github.com/prometheus/tsdb/tree/master/docs/format
[tsdb-talk]: https://www.slideshare.net/FabianReinartz/storing-16-bytes-at-scale-81282712
[tsdb-lib]: https://godoc.org/github.com/prometheus/tsdb
[promql-lib]: https://godoc.org/github.com/prometheus/prometheus/promql
[prom-http-api]: https://prometheus.io/docs/querying/api/

