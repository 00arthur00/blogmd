title: 存储引擎概览
tags:
  - m3db
  - 时间序列数据库
categories: []
date: 2019-05-16 14:29:00
---
M3DB是一个时间序列数据库，主要设计为可水平扩展并处理大量监视时间序列数据。

## 时间序列压缩(M3TSZ)

M3DB作为时间序列数据库的最大优势之一（与使用更通用的水平可扩展分布式数据库，如Cassandra相反）是其压缩时间序列数据的能力，从而节省了大量内存和磁盘。 这种高压缩比是通过M3TSZ算法实现的，这是[**Facebook的Gorilla论文**](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)中描述的流时间序列压缩算法的一种变体，但有一些小差别。

压缩比将根据工作负载和配置而有所不同，但我们发现使用M3TSZ，我们能够实现与Uber生产工作负载的**1.45字节/数据点**的压缩比。这比标准TSZ提高了40％，在相同条件下仅给出了2.42字节/数据点的压缩比。

## 架构
M3DB是一个具有持久存储的持久数据库，但最好通过其**内存中对象布局和磁盘表示**之间的边界来理解。

## 内存布局

```
                   ┌───────────────────────────────┐
   ┌───────────────┤           Database            ├─────────────────┐
   │               └───────────────────────────────┘                 │
   │                                                                 │
   │                                                                 │
   │                                                                 │
   │               ┌───────────────────────────────┐                 │
   │     ┌─────────┤          Namespace 1          ├──────────┐      │
   │     │         └───────────────────────────────┘          │      │
   │     │                                                    │      │
   │     │                                                    │      │
   │     │                   ┌───────────┐                    │      │
   │     │    ┌──────────────┤  Shard 1  ├──────────────┐     │      │
   │     │    │              └───────────┘              │     │      │
   │     │    │                                         │     │      │
   │     │    │                                         │     │      │
   │     │    │              ┌───────────┐              │     │      │
   │     │    │   ┌──────────┤ Series 1  ├──────────┐   │     │      │
   │     │    │   │          └───────────┘          │   │     │      │
   │     │    │   │                                 │   │     │      │
   │     │    │   │                                 │   │     │      │
   │     │    │   │ ┌─────────────────────────────┐ │   │     │      │
   │     │    │   │ │      Block [2PM - 4PM]      │ │   │     │      │
   │     │    │   │ ├─────────────────────────────┤ │   │     │      │
   │     │    │   │ │      Block [4PM - 6PM]      │ │   │     │      │
   │     │    │   │ ├─────────────────────────────┤ │   │     │      │
   │     │    │   │ │       ┌────────────┐        │ │   │     │      │
   │     │    │   │ └───────┤   Blocks   ├────────┘ │   │     │      │
   │     │    │   │         └────────────┘          │   │     │      │
   │     │    │   │                                 │   │     │      │
   │     │    │   │                                 │   │     │      │
   │     │    │   │  ┌────────────────────────────┐ │   │     │      │
   │     │    │   │  │                            │ │   │     │      │
   │     │    │   │  │     Block [6PM - 8PM]      │ │   │     │      │
   │     │    │   │  │                            │ │   │     │      │
   │     │    │   │  ├────────────────────────────┤ │   │     │      │
   │     │    │   │  │ Active Buffers (encoders)  │ │   │     │      │
   │     │    │   │  └────────────────────────────┘ │   │     │      │
   │     │    │   │                                 │   │     │      │
   │     │    │   │                                 │   │     │      │
   │     │    │   └─────────────────────────────────┘   │     │      │
   │     │    │                                         │     │      │
   │     │    │                                         │     │      │
   │     │    │                                         │     │      │
   │     │    │                                         │     │      │
   │     │    └─────────────────────────────────────────┘     │      │
   │     │                                                    │      │
   │     │                                                    │      │
   │     └────────────────────────────────────────────────────┘      │
   │                                                                 │
   └─────────────────────────────────────────────────────────────────┘
```

M3DB的内存部分是通过对象层次结构实现的：
1. **每个M3DB进程只有一个`database`**。
2. 数据库“拥有”许多名称空间，每个名称空间都有一个唯一的名称以及关于数据保留期和块大小的不同配置（我们将在后面详细讨论）。**命名空间与其他数据库中的表类似**。
3. 命名空间拥有的分片(shard)。分片实际上与Cassandra中的“虚拟分片”相同，因为它们通过系列ID的简单散列提供时间序列数据的任意分布。
4. 序列由分片拥有。当您想到“时间序列”数据时，通常会想到一个系列。例如:一段时间内数据中心中单个主机的CPU级别可以表示为id为“.system.cpu.utilization”的序列和（TIMESTAMP，CPU_LEVEL）形式的元组向量。换句话说，如果您渲染图形，则序列将表示该图形上的单个线条。请注意，前面的示例只是一个逻辑说明，并不代表M3DB实际存储数据的方式。
5. *块属于一系列，是M3DB设计的核心*。块只是密封（不再可写）压缩时间序列数据流周围的较小包装器对象。压缩有一些注意事项，即您无法读取压缩块中的单个数据点。换句话说，**为了读取单个数据点，您必须将整个块解压缩到您尝试读取的数据点**。

如果M3DB将所有内容保存在内存中（事实上，它的早期版本确实是这么做的），那么您可以从概念上将其视为由地图层次结构组成：
```
database_obect      => map<namepace_name, namespace_object>
namespace_object    => map<shard_id, shard_object>
shard_object        => map<series_id, series_object>
series_object       => map<block_start_time, block_object>
series_object       => map<block_start_time, active_buffers(encoders)> (This map should only have one or two entries)
```

### 持久存储
虽然内存数据库可能很有用（并且M3DB支持在仅内存模式下运行），但持久性需要某种形式的持久性。 换句话说，如果没有持久性策略，M3DB就不可能在不丢失所有数据的情况下重新启动（或从崩溃中恢复）。

此外，对于大量数据，将所有数据保存在内存中变得非常昂贵。对于监控工作负载尤其如此，这些工作负载通常遵循**“一次写入，从不读取”**模式，其中所存储的所有数据中只有不到百分之几被读取。 使用这种类型的工作负载时，如果可以将所有数据保存在磁盘上并在需要时检索，则将所有数据保存在内存中显得太浪费了。

与大多数其他数据库一样，M3DB采用双管齐下的持久存储方法，包括将**commitlog（用于灾难恢复）与定期快照（用于高效检索）相结合**：
1. 所有写入都持久保存到[commitlog](https://m3db.github.io/m3/m3db/architecture/commitlogs/)（commitlog可以配置为fsync每次写入，或者可选地批量一起写入，速度快得多，但在发生灾难性故障的情况下可能会丢失少量数据）。**commitlog完全未压缩，仅在数据库关闭（有意或无意）的情况下恢复“未刷新”数据，并且永远不会用于满足读取请求。**
2. 定期（基于配置的块大小）所有“活动”块被“密封”（标记为不可变）并作为“[文件集](https://m3db.github.io/m3/m3db/architecture/storage/)”文件刷入(flush)到磁盘。 这些文件是高度压缩的，可以通过其互补索引文件编入索引。查看刷入(flush)部分，了解有关[刷入(flush)过程](https://m3db.github.io/m3/m3db/architecture/engine/#flushing)背景的更多信息。

**blocksize参数是需要针对特定工作负载进行调整的最重要变量**。较小的块大小意味着更频繁的刷新，并且正在被主动压缩的数据的内存占用更少，但它也会降低压缩比，并且您的数据将占用磁盘上更多的空间。

如果数据库由于“刷入”（将文件集文件写入磁盘）之间的任何原因而停止，那么当节点启动备份时，需要通过读取提交日志或来自对等方的数据流来恢复这些写入 负责相同的分片（如果复制因子大于1）。

虽然[文件集文件](https://m3db.github.io/m3/m3db/architecture/storage/)旨在通过序列主键（ID）支持高效的数据检索，但是任何必须从磁盘检索数据的查询仍然存在沉重的成本，因为转到磁盘总是比访问主内存慢得多。**为了弥补这一点，M3DB支持各种缓存策略，这些策略可以通过缓存内存中的数据来显着提高读取性能**。

## 写入路径

我们现在有足够的M3DB架构上下文来讨论写入的生命周期。当M3DB客户端调用M3DB的嵌入式thrift服务器上的[writeBatchRaw](https://github.com/m3db/m3/blob/06d3ecc94d13cff67b82a791271816caa338dcab/src/dbnode/generated/thrift/rpc.thrift#L59)端点时，写入开始。写本身将包含以下信息：
1. 命名空间
2. 系列ID（字节blob）
3. 时间戳
4. value值本身

M3DB将查询数据库对象以检查命名空间是否存在，如果存在，则它将散列序列ID以确定它属于哪个分片。如果接收write的节点拥有该分片，那么它将在分片对象中查找该序列。如果序列存在，那么它将查找序列的相应编码器并将数据点编码到**压缩流**中。如果编码器不存在（此系列的编码尚未作为此块的一部分发生），则将分配新的编码器，它将开始具有该数据点的压缩M3TSZ流。还有一些用于处理**无序写入**的特殊逻辑，在[合并所有编码器](https://m3db.github.io/m3/m3db/architecture/engine/#merging-all-enoders)部分中讨论。

同时，写入将附加到commitlog队列（并且取决于commitlog配置，立即fsync到磁盘或与其他写入一起批处理并立即刷新）。

写入将仅存在于此“活动缓冲区”和commitlog中，直到块结束并刷入到磁盘，此时写入将存在于文件集文件中，以便以后进行有效存储和检索，并且可以对commitlog条目进行垃圾回收。

注意：无论在单个节点中写入成功与否，客户端都将根据配置的[一致性级别](https://m3db.github.io/m3/m3db/architecture/consistencylevels/)向调用方返回成功或失败。

## 读取路径

当M3DB客户端调用M3DB嵌入式thrift服务器上的[FetchBatchResult](https://github.com/m3db/m3/blob/master/src/dbnode/generated/thrift/rpc.thrift)或[FetchBlocksRawResult](https://github.com/m3db/m3/blob/master/src/dbnode/generated/thrift/rpc.thrift)端点时，将开始读取。读取请求将包含以下信息：
1. 命名空间
2. 序列ID（字节blob）
3. 请求的时间段（开始和结束）

M3DB将查询数据库对象以检查命名空间是否存在，如果存在，则**它将散列系列ID以确定它属于哪个分片**。如果接收读取的节点拥有该碎片，那么M3DB需要确定两件事：
1. 该系列是否存在？如果确实如此
2. 数据是存在于“活动缓冲区”中（由编码器主动压缩），缓存在内存中，磁盘上还是三者的某种组合？

确定系列是否存在很简单。M3DB在分片对象中查找该系列。如果存在，则系列存在。如果没有，则M3DB查询该分片/块启动组合的内存布隆过滤器，以确定该系列是否存在于磁盘上。

如果系列存在，那么对于请求跨越的每个块，M3DB需要合并来自活动缓冲区，内存高速缓存和文件集文件（磁盘）的数据。

让我们设想一个读取给定系列请求最后6个小时的数据，以及一个配置了2个小时的块大小的M3DB命名空间（即我们需要找到3个不同的块）。

如果当前时间是晚上8点，则所请求的块的位置可能如下所示：
```
[2PM - 4PM (FileSet file)]    - 已密封并刷入磁盘但未缓存
[4PM - 6PM (In-memory cache)] - 已密封并刷入磁盘且缓存
[6PM - 8PM (active buffer)]   - 未密封、未刷入存盘
```
那么M3DB需要合并:
1. 来自活动缓冲区/编码器的尚未密封的块（位于Series对象内部查找内）**[6PM-8PM]**
2. 内存中缓存块(也位于Series对象内部查找内）**[4PM-6PM]**
3. 来自磁盘的块(从磁盘检索的块将根据当前的缓存策略进行缓存**)[2PM-4PM]**


从活动缓冲区和内存缓存中检索块很简单，数据已经存在于内存中，并且可以通过序列ID键入的哈希映射轻松访问。从磁盘检索块更复杂。从磁盘检索块的流程如下：

1. 查询内存中的bloom过滤器，以确定该系列是否存在于磁盘上。
2. **如果bloom过滤器返回存在，则二进制搜索内存中索引摘要以查找我们正在搜索的序列ID之前的最近索引条目。查看index_lookup.go文件以获取实现详细信息。**
3. 跳转到我们在上一步中从二进制搜索中获得的索引文件中的偏移量，并开始向前扫描，直到我们识别出我们正在寻找的系列ID的索引条目，或者我们在索引文件中得到足够远以确认我们正在寻找的ID不存在（这是可能的，因为索引文件按ID排序）
4. 跳转到我们在上一步中扫描索引文件获得的数据文件中的偏移量，并开始流式传输数据。

一旦M3DB从内存/磁盘上各自的位置检索到三个块，它就会将所有数据传回客户端。**客户端是否向调用者返回读取成功取决于配置的[一致性级别](https://m3db.github.io/m3/m3db/architecture/consistencylevels/)。**

注意：**由于M3DB节点返回压缩块（M3DB客户端对它们进行解压缩），因此无法为给定块返回“部分结果”。 如果读取请求的任何部分跨越给定块，则该块的整体必须被传送回客户端。实际上，由于M3DB能够实现高压缩比，因此最终不会成为问题。**

## 后台进程
M3DB具有在正常操作期间在后台运行的各种进程。

### 滴答
滴答进程在后台持续运行，负责各种任务：
1. 合并给定系列/块起始组合的所有编码器
2. 从内存中删除过期/刷入的序列和块
3. 从文件系统中清除过期数据（文件集/提交日志）

#### 合并所有编码器
M3TSZ设计用于压缩时间序列数据，其中每个数据点的时间戳大于最后编码的数据点。 对于监视工作负载，这非常有效，因为每个后续数据点几乎总是在前一个数据点之后按时间顺序排列。 但是，现实世界的系统很乱，偶尔会收到无序写入。 发生这种情况时，M3DB将为无序数据点分配新的编码器。 在将数据刷新到磁盘之前需要合并多个编码器，但为了防止在刷新过程中出现大量内存峰值，我们会在后台不断合并乱序编码器。

#### 从内存中删除过期/刷入的序列和块
根据配置的缓存策略，内存中对象布局最终可能会引用已过期的系列或数据块（已超出保留期）或不再需要在内存中（由于数据被刷入）到磁盘或不再需要缓存）。后台滴答进程将识别这些结构并从内存中释放它们。

### 刷入
正如[架构部分](https://m3db.github.io/m3/m3db/architecture/engine/#architecture)所讨论的那样，写入在内存中被主动缓冲/压缩，并且持续写入提交日志，但最终需要以[文件集文件](https://m3db.github.io/m3/m3db/architecture/storage/)的形式将数据刷入到磁盘，以便于高效存储和检索。

这是可配置的“blocksize”发挥作用的地方。块大小只是一段时间，它指示在“密封”（标记为不可变）并刷新到磁盘之前，在内存中压缩活动写入的时间（以流方式）。让我们使用两个小时的块大小作为例子。

如果blocksize设置为两个小时，则给定分片的所有序列的所有写入将一次缓冲在内存中两个小时。在两小时结束时，将生成所有文件集文件，写入磁盘，然后可以释放内存中的对象并替换为新块的新文件。旧对象将在后续滴答从内存中删除。

## 警告/限制

1. 目前，M3DB不支持任意更新或删除。在可变块中支持时间序列数据的upsert，但是在时间序列数据移出可变窗口之后，它变得不可变。
2. M3DB不支持任意写入过去和未来。这对于监视工作负载通常很好，但对于传统的[OLTP](https://en.wikipedia.org/wiki/Online_transaction_processing)和[OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing)工作负载可能会有问题。未来版本的M3DB将更好地支持具有任意时间戳的写入。
3. M3DB不支持使用双精度浮点数以外的值写入数据点。未来版本的M3DB将支持存储任意值。
4. M3DB不支持以无限期保留期存储数据，M3DB中的每个命名空间都需要具有保留策略，该策略指定将保留该命名空间中的数据的时间长度。虽然该值没有上限（Uber的生产数据库运行时保留期高达5年），但仍然需要并且一般来说M3DB针对具有明确定义的[TTL](https://en.wikipedia.org/wiki/Time_to_live)的工作负载进行了优化。
5. M3DB不支持后台数据修复或Cassandra式[读取修复](https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/opsRepairNodesReadRepair.html)。未来版本的M3DB将支持作为持续运行的后台进程的数据自动修复。